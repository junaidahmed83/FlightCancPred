{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight cancellation predictions\n",
    "   ### Study by  Junaid Shaikh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The airline delays and cancellations every year significantly impact both airlines and their passengers. According to an estimate, only airline delays cost $22 billions annually to the airlines. The delays and cancellations also affect negatively on the capacity of airports as well on the goodwill of airline passengers. To be able to reduce the impact of such events and devise better and timely actions, prediction of delays and cancellations are important to airlines, airports and passengers.\n",
    "\n",
    "In this work, I am initiating a short study on the prediction of airline cancellations and present some preliminary insights based on a US airline dataset available on the following link: \n",
    "http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time\n",
    "\n",
    "My insights are based on the data of 19 months (from 01/2015 to 07/2016). The dataset is very interesting. The problem became interesting as study progressed. However, This report is unfortunately limited to the study of only a few aspects. There are several aspects which could be explored further. I will discuss them further at the end of this report.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] https://books.google.se/books?id=4yOeWfX5gekC&pg=PA16&redir_esc=y#v=onepage&q&f=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "The dataset consists of 9.09 million rows. The number of variables are limited to 37, which are listed below. This means that there were 9.09 million flights between 1 Jan 2015 to 31 July 2016. These flights belong to 14 airline companies and connected passengers between 323 airports within the US. A total of 5061 tail numbers were used for 9.09 million flights. \n",
    "\n",
    "*The tail number is the aircraft registration number. Tail numbers are useful (among other things) for tracking private aircraft and non-scheduled charter flights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import pickle\n",
    "import json\n",
    "import forecastio\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "import forecastio\n",
    "import getpass\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_pickle(\"raw_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print raw_data.shape\n",
    "for col in raw_data.columns:\n",
    "    print col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of unique airline IDs:\",len(raw_data[\"AIRLINE_ID\"].unique())\n",
    "print \"Unique carriers:\", raw_data[\"UNIQUE_CARRIER\"].unique()\n",
    "print \"Number of unique tail numbers:\", len(raw_data[\"TAIL_NUM\"].unique())\n",
    "print raw_data[\"TAIL_NUM\"].head(5)\n",
    "print raw_data[\"FL_NUM\"].head(5)\n",
    "print raw_data[\"FLIGHTS\"].head(5)\n",
    "print \"Number of unique number of flights:\", len(raw_data[\"FLIGHTS\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Number of unique origin airports:\",len(raw_data[\"ORIGIN\"].unique())\n",
    "print \"Number of unique destination airports:\",len(raw_data[\"DEST\"].unique())\n",
    "print \"Airport examples:\", raw_data[\"ORIGIN\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airline performance factors\n",
    "\n",
    "The data suggest that there are several different factors that may affect the airline performance. These factors may broadly be categorised into temporal, spatial and environmental factors. In this study, mainly these three dimensions are further taken into account. First we will take a brief look at the airline performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance \n",
    "The performance of flights can be analysed from three perspective:\n",
    "\n",
    "* Time delays\n",
    "* Cancellations  \n",
    "* Diversions\n",
    "\n",
    "According to Federal Aviation Administration (FAA), the flights which are delayed more than 15 minutes from their scheduled time are considered \"delayed\" flights. Similarly, the airlines which do not fly at all from their respective origins are considered \"cancelled\" flights.\n",
    "\n",
    "We will now study cancelled flights in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flight cancellations\n",
    "A total of 9098999 flights were scheduled between 2015/01/01 and 2016/07/31. Among these flights, 134014 flights were cancelled. This makes a cancellation rate of 1.4%. Among cancelled flights, 54.1% were cancelled due to weather reasons, according to data. 28.2% flights were cancelled due to carrier, 17.5% due to National Airspace System (NAS) and only 0.01% were cancelled due to the security reasons.\n",
    "\n",
    "A => Carrier ; B => Weather ; C => NAS; D => Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Total number of flights:\", raw_data['CANCELLED'].count()\n",
    "print \"Normal and cancelled flights:\"\n",
    "print raw_data['CANCELLED'].value_counts()\n",
    "print raw_data['CANCELLATION_CODE'].value_counts()\n",
    "print raw_data['CANCELLATION_CODE'].value_counts()/134014\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancellation ratio by time\n",
    "\n",
    "Cancellation ratio is higher on Mondays (2.06%) as compared to the other days of week. The cancellation ratio is lowest on Fridays. Months of January and February are relatively more prone to flight cancellations. The ratio of cancelled flights go over 3% in February. December, June and July are the other main candidates for most flight cancellations. Higher cancellations in January and February can be intuitively attributed to bad weather in these winter months in North America. Cancellations in December, June and July may be explained by extra load and lack of resources due to vacation and tourism periods.\n",
    "\n",
    "Moreover, in terms of time of day, cancellations are relatively higher at a few selected hours in a day, e.g., in the evenings between 18 to 21 when the cancellations are 1.8% or above. Similarly, cancellation ratio appears to be high at 2 am in the morning, which is quite surprising. Bad weather or poor visibility could be a probable cause behind such delays. However, the cancellation ratio seems to dive sharply after 2 am. \n",
    "\n",
    "1-7 => Monday - Sunday\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "ax = raw_data.groupby('DAY_OF_WEEK')['CANCELLED'].agg(\"mean\").plot(style='-o', grid=1, xticks=np.arange(1,8,1), figsize=(12,5),label='Cancellatio Ratio')\n",
    "ax.set_ylabel(\"Cancellation Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = raw_data.groupby('MONTH')['CANCELLED'].agg(\"mean\").plot(style='-o', label = 'CD scale',grid=1, figsize=(12,5),color='g')\n",
    "ax.set_ylabel(\"Cancellation Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "raw_data[\"CRS_DEP_TIME_HOUR\"] = (raw_data[\"CRS_DEP_TIME\"]/100).apply(np.floor).astype(int)\n",
    "raw_data[\"CRS_ARR_TIME_HOUR\"] = (raw_data[\"CRS_ARR_TIME\"]/100).apply(np.floor).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data[[\"CRS_DEP_TIME\",\"CRS_DEP_TIME_HOUR\",\"CRS_ARR_TIME\",\"CRS_ARR_TIME_HOUR\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#raw_data.groupby('CRS_DEP_TIME_HOUR')['CANCELLED'].agg(\"mean\").plot(figsize=(15,5))\n",
    "ax =  raw_data.groupby('CRS_DEP_TIME_HOUR')['CANCELLED'].agg(\"mean\").plot(color = 'b',grid=1, style='-o', xticks=np.arange(0,24,1), figsize=(12,5))\n",
    "ax.set_ylabel(\"Cancellation Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancellation ratio by carrier\n",
    "\n",
    "Let's look at the cancellation ratio by carriers. The cancellations very quite a lot across carriers (upto 10 fold). From 5% to less than 0.4%. The potential reason is that carriers with very high cancellatio ratio are not very top-tier passenger carriers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = raw_data.groupby('UNIQUE_CARRIER')['CANCELLED'].agg(\"mean\").sort_values(ascending=0).plot(kind = 'bar',figsize=(12,5))\n",
    "ax.set_ylabel(\"Cancellation Ratio\")\n",
    "ax.set_xlabel(\"Carrier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancellations by airport\n",
    "There are 323 airports. The following bar plot shows top 20% origin airports by cancellation ratio. The cancellation ratio for these airports is above 2% and usually below 10%. However, the ratio is even above 15% for one airport (MMH). The potential reason behind high cancellation ratio could be that these airports are not very popular airports. Therefore, generally there are very few airlines that fly from these airports. Therefore, we need to look at the absolute number of cancelled flights. \n",
    "\n",
    "The number of cancelled flights show that the popular airports are among the top airports by absolute number of flight cancellations. Chicago o'Hare Domestic (ORD) airport, which is among one of the most popular airports tops the list of most cancelled flights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Top 64 (~20%) airports w.r.t cancellation ratio of flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = raw_data.groupby('ORIGIN')['CANCELLED'].agg(\"mean\").sort_values(ascending=0)[:64].plot(kind = 'bar',figsize=(15,5), color ='g')\n",
    "ax.set_ylabel(\"Cancellation Ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 64 (~20%) airports w.r.t cancellation numbers of flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = raw_data.groupby('ORIGIN')['CANCELLED'].agg(\"sum\").sort_values(ascending=0)[:64].plot(kind = 'bar',figsize=(15,5), color ='b')\n",
    "ax.set_ylabel(\"Number of Cancellations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = raw_data.groupby('ORIGIN')['CANCELLED'].value_counts()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_2 = pd.DataFrame(temp)\n",
    "temp_2 = temp_2.rename(columns={\"CANCELLED\":\"COUNTS\"}) \n",
    "temp_2 = temp_2.reset_index(level=['ORIGIN','CANCELLED'])\n",
    "temp_2 = pd.pivot_table(temp_2,index=[\"ORIGIN\"],columns=[\"CANCELLED\"],values=[\"COUNTS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 25 airports w.r.t cancellation numbers along with non-cancelled flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_2['COUNTS'][1] \n",
    "ax = temp_2.sort_values([('COUNTS',1)], ascending=0)[:25].plot(kind=\"bar\",figsize=(15,5))\n",
    "ax.set_ylabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described at the start of this report that the main aim of this study is to develop a model that can help in the prediction of cancelled flights. However, this raises further practical questions:\n",
    "\n",
    "* What should model exactly predict? \n",
    "\n",
    "   - Should we Predict if a flight will be cancelled or not? (Binary classification problem) OR\n",
    "   \n",
    "   - Should we Predict of number of cancellations within a certain interval? (Regression problem)\n",
    "\n",
    "In this study we will stick to the former one, i.e. the binary classification problem.\n",
    "\n",
    "* How far in advance before the original flight departure time should the prediction be made in number of hours?\n",
    "\n",
    "This can be decided based on evaluation of model with different number of hours. However, due to time limitation, in this report predictions are made 3 hours in advance. The 3 hours time limit is chosen based on the fact that domestic passengers usually arrive at airports less than 3 hours before departure time. Similarly gate allotment to flights at airport is done around 3 hours before departure time.\n",
    "\n",
    "Thus, if T_d  is the original departure time, T_p = (T_d - 3 hours) is the prediction time. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study, feature extraction is performed in three areas based on the exploration of data:\n",
    "\n",
    "* Carrier-related features\n",
    "* Airport-related features\n",
    "* Weather-related features\n",
    "* Temporal features\n",
    "* Cancellations of carrier and origin in the recent time \n",
    "\n",
    "Cancellation in recent time can be done by time-windowing and estimating the numbers. To perform time-windowing per origin, destination and carrier level, I briefly switch to the Spark dataframes. The spark time window functions are quite nice (inherited from SQL) and help perform operations on timewindows in fast and easy way. (copy my script from Appache Zeppelin here)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "import math\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "raw_data_spark_df = sqlc.createDataFrame(raw_data)\n",
    "\n",
    "### windowing according to carrier, origin and destination \n",
    "## Time window length: 2 hours i.e. T_p - 2hrs  to T_p\n",
    "\n",
    "windowSpec_carrier = Window.partitionBy(raw_data_spark_df['UNIQUE_CARRIER']).orderBy(raw_data_1_df['CRS_DEP_EPOCH_TS'].asc())\n",
    "cancelled_flights = func.sum(raw_data_spark_df['CANCELLED']).over(windowSpec_carrier.rangeBetween(-18000, -7200))\n",
    "\n",
    "windowSpec_origin = Window.partitionBy(raw_data_spark_df['ORIGIN']).orderBy(raw_data_spark_df['CRS_DEP_EPOCH_TS'].asc())\n",
    "cancelled_flights_origin = func.sum(rraw_data_spark_df['CANCELLED']).over(windowSpec_origin.rangeBetween(-18000, -7200))\n",
    "\n",
    "windowSpec_dest = Window.partitionBy(raw_data_spark_df['DEST']).orderBy(raw_data_spark_df['CRS_DEP_EPOCH_TS'].asc())\n",
    "cancelled_flights_dest = func.sum(raw_data_spark_df['CANCELLED']).over(windowSpec_dest.rangeBetween(-18000, -7200))\n",
    "\n",
    "windowed_data = raw_data_spark_df.select(\"*\",cancelled_flights.alias(\"CARRIER_CANCELLED\"),cancelled_flights_origin.alias(\"ORIGIN_CANCELLED\"),cancelled_flights_dest.alias(\"DEST_CANCELLED\"))\n",
    "\n",
    "windowed_data_features = windowed_data.select(\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DAY_OF_WEEK\",\"UNIQUE_CARRIER\",\n",
    "                                              \"TAIL_NUM\",\"ORIGIN\",\"DEST\",\"CRS_ELAPSED_TIME\",\"DISTANCE\",\n",
    "                                              \"CRS_DEP_TIME_HOUR\",\"CRS_ARR_TIME_HOUR\",\"CARRIER_CANCELLED\",\n",
    "                                              \"ORIGIN_CANCELLED\",\"DEST_CANCELLED\",\"CANCELLED\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Indexing of string-related features (to have numeric data). \n",
    "# Columns used for indexing: \"UNIQUE_CARRIER\",\"TAIL_NUM\",\"ORIGIN\",\"DEST\"\n",
    "indexers_flight = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(test_features_1) for column in [\"UNIQUE_CARRIER\",\"TAIL_NUM\",\"ORIGIN\",\"DEST\"]]\n",
    "\n",
    "pipeline_flight = Pipeline(stages=indexers_flight)\n",
    "flight_features_1_indexed = pipeline_flight.fit(windowed_data_features).transform(windowed_data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model development and evaluation (Rough & Quick)\n",
    "\n",
    "Now we may move on to the model development and evaluation process. To be able to keep up with time, I am reducing the dataset to only one origin airport, i.e, the model development will be done for flights departing from only one airport in the US. As we saw previously that Chicago ORD airport has highest cancellation ratio (2.4%) among the popular airports, we will reduce our dataset to flights with ORD airport as origin. \n",
    "\n",
    "Although, ORD airport has higher cancellation ratio among popular airports, the cancellation ratio of 2.4% is still quite low for a binary classification problem. The rarety of cancellation events make this highly class-imbalanced problem with a large number (97.8% 0s) of non-cancelled flights as compared to cancelled flights (2.4% 1s). Hence, we will have to take lot of caution while evaluating our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## returning back from spark DF, now we have pandas DF at hand: flight_features_1_indexed_ord_2_pd\n",
    "print flight_features_1_indexed_ord_2_pd.shape\n",
    "flight_features_1_indexed_ord_2_pd.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these work, I have limited the model development to:\n",
    "* Linear Regression\n",
    "* Logistic Regression\n",
    "* K Nearest Neighbors (KNN)\n",
    "* Random Forest\n",
    "\n",
    "I wanted to test xgboost and SVM too but due to time limitation, I am limiting the evaluations to the above algorithms. \n",
    "Just to have quick idea, the evaluations are limited to only one run to save time. This may however, make the models biased towards the splits between training and validation samples.\n",
    "\n",
    "I played around little bit with the different parameters within the above models, however, there was no dramatic differences due to the parameters.\n",
    "\n",
    "For evaluations, 80% samples are randomly selected for training and 20% for the validation. This may potentially create information leakage problem as samples are evluated independent of time. However, just to have sneakpeak at model performance, I stick to random evaluation. Later, I will show some evaluations by splitting training and test samples keeping timeline in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below first round of model development and evaluation shows that Random Forest (RF) model outperforms other models (which is not surprising in this case). However, the accuracy of predicting cancelled flights is still not very high. Particularly Recall score for cancelled flights is very low. The highest recall score is 0.40, which we achieve using RF with 50 trees. F1-score for RF is however above 50%. r2_score is relatively higher in RF model.\n",
    "\n",
    "This shows that class imbalance problem has a large influence on the model. The model frequently predicts that flight will not be cancelled even if it is cancelled in reality. Thus recall, needs to be improved to have better predictions of cancelled flights. On the other Precision scores are quite high. This means that the false positives are rare, which is good because if a model predicts that flight will be cancelled and in reality it does not  then these false alarms can result in airlines or passengers not being ready for the flight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = flight_features_1_indexed_ord_2_pd.ix[:,0:14]\n",
    "labels = flight_features_1_indexed_ord_2_pd.ix[:,14:15]\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Linear regression\n",
    "modelFit_sgd = linear_model.SGDClassifier().fit(X_train,y_train)\n",
    "#score =  modelFit.score(X_test, y_test)\n",
    "predictions_sgd = modelFit_sgd.predict(X_test)\n",
    "confMatrix_sgd = confusion_matrix(y_test, predictions_sgd)\n",
    "classReport_sgd = classification_report(y_test, predictions_sgd)\n",
    "r2_sgd = r2_score(y_test, predictions_sgd)\n",
    "accuracy_sgd = accuracy_score(y_test, predictions_sgd)\n",
    "#precision_1.append()\n",
    "    \n",
    "\n",
    "print \"Confusion Matrix:\\n\",confMatrix_sgd \n",
    "print \"Classification Report:\\n\",classReport_sgd\n",
    "print \"r2_score: \", r2_sgd\n",
    "print \"accuracy score: \", accuracy_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelFit_lr = linear_model.LogisticRegression().fit(X_train,y_train)\n",
    "predictions_lr = modelFit_lr.predict(X_test)\n",
    "confMatrix_lr = confusion_matrix(y_test, predictions_lr)\n",
    "classReport_lr = classification_report(y_test, predictions_lr)\n",
    "r2_lr = r2_score(y_test, predictions_lr)\n",
    "accuracy_lr = accuracy_score(y_test, predictions_lr)\n",
    "\n",
    "print \"Confusion Matrix:\\n\",confMatrix_lr \n",
    "print \"Classification Report:\\n\",classReport_lr\n",
    "print \"r2_score: \", r2_lr\n",
    "print \"accuracy score: \", accuracy_lr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelFit_knn = KNeighborsClassifier(n_neighbors=10, weights='distance').fit(X_train,y_train)\n",
    "\n",
    "predictions_knn = modelFit_knn.predict(X_test)\n",
    "confMatrix_knn = confusion_matrix(y_test, predictions_knn)\n",
    "classReport_knn = classification_report(y_test, predictions_knn)\n",
    "r2_knn = r2_score(y_test, predictions_knn)\n",
    "accuracy_knn = accuracy_score(y_test, predictions_knn)\n",
    "\n",
    "print \"Confusion Matrix:\\n\",confMatrix_knn \n",
    "print \"Classification Report:\\n\",classReport_knn\n",
    "print \"r2_score: \", r2_knn\n",
    "print \"accuracy score: \", accuracy_knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelFit_rf = RandomForestClassifier(n_estimators=50,max_features='auto',max_depth=None,min_samples_leaf=1).fit(X_train,y_train)\n",
    "predictions_rf = modelFit_rf.predict(X_test)\n",
    "confMatrix_rf = confusion_matrix(y_test, predictions_rf)\n",
    "score_rf =  modelFit_rf.score(X_test, y_test)\n",
    "classReport_rf = classification_report(y_test, predictions_rf)\n",
    "r2_rf = r2_score(y_test, predictions_rf)\n",
    "accuracy_rf = accuracy_score(y_test, predictions_rf)\n",
    "\n",
    "print \"Confusion Matrix:\\n\",confMatrix_rf \n",
    "print \"Classification Report:\\n\",classReport_rf\n",
    "print \"r2_score: \", r2_rf\n",
    "print \"accuracy score: \", accuracy_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw previously that a large proportion of flights are cancelled due to the weather problems. Therefore, it is important that we include weather data at the time of prediction as features in the model. \n",
    "\n",
    "The following snippet uses DarkSky API to download weather data for airport of origin, i.e., ORD. For the time being, we disregard downloading of weather data for destination airports. Again the main issue is unavailability of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import forecastio\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "'''\n",
    "Downloading just for one airport. Could be extended to multiple airports by merging location dataset of aiports \n",
    "with corresponding coordinates\n",
    "'''\n",
    "## Downloading 19 months of hourly weather data \n",
    "\n",
    "i=0\n",
    "origin_ord_weather = pd.DataFrame(columns=('w_origin','w_timestamp','temperature', 'windSpeed', 'cloudCover', 'precipProbability'))\n",
    "ORIGINS = [\"ORD\"]\n",
    "lat = 41.9 \n",
    "lng = -87.9\n",
    "start = datetime(2015,1,1)\n",
    "print \"start\"\n",
    "for origin in ORIGINS:\n",
    "    months_19 = start + relativedelta(months=+19)#timedelta(month=1)\n",
    "    count=0\n",
    "    for dt in rrule.rrule(rrule.DAILY, dtstart=start, until=months_19):\n",
    "        count=count+1\n",
    "        forecast = forecastio.load_forecast(api_key, lat, lng,dt,units=\"uk2\")\n",
    "        for data in forecast.json[\"hourly\"][\"data\"]:\n",
    "            origin_ord_weather.loc[i]= [origin,data.get(\"time\"),data.get(\"temperature\"),data.get(\"windSpeed\"),data.get(\"cloudCover\"),data.get(\"precipProbability\")]\n",
    "            i=i+1    #    print data.get(\"visibility\")\n",
    "            #print forecast.json[\"hourly\"]\n",
    "print \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin_ord_weather.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging weather and flight datasets on prediction timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_features_1_indexed_ord_2_pd[\"EPOCH_TS_PRED\"] = flight_features_1_indexed_ord_2_pd[\"EPOCH_TS\"] - 10800 # prediction timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_test = flight_features_1_indexed_ord_2_pd.merge(origin_ord_weather,left_on='EPOCH_TS_PRED',right_on='w_timestamp',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting final features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_test = pd.DataFrame()\n",
    "final_feature_test = merge_test[['YEAR','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','CRS_DEP_TIME_HOUR','CRS_ARR_TIME_HOUR','UNIQUE_CARRIER_index','TAIL_NUM_index','ORIGIN_index','ORIGIN_CANCELLED','DEST_index','DISTANCE','DEST_CANCELLED','CARRIER_CANCELLED','temperature','windSpeed','cloudCover','precipProbability','CANCELLED']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling missing values with mean of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_feature_test['cloudCover'] = final_feature_test['cloudCover'].fillna(np.mean(final_feature_test['cloudCover']))\n",
    "final_feature_test['precipProbability'] = final_feature_test['precipProbability'].fillna(np.mean(final_feature_test['precipProbability']))\n",
    "final_feature_test['windSpeed'] = final_feature_test['windSpeed'].fillna(np.mean(final_feature_test['windSpeed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = final_feature_test.ix[:,0:18]\n",
    "labels = final_feature_test.ix[:,18:19]\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model development & evaluation -  Phase 2\n",
    "After including weather-related features, the accuracy of model improved slightly. Particularly, the Recall score increased at the expense of Precision. Thus, number of correct predictions of cancelled flights went above 1000, however, still under 50% mark. This means the model is still predicting more frequently that the flights will not be cancelled. \n",
    "\n",
    "The weather-related features are only included for Origin and not the destination. Including weather-related features for destination airports may improve the performance of model significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelFit_rf = RandomForestClassifier(n_estimators=50,max_features='auto',max_depth=None,min_samples_leaf=1).fit(X_train,y_train)\n",
    "predictions_rf = modelFit_rf.predict(X_test)\n",
    "confMatrix_rf = confusion_matrix(y_test, predictions_rf)\n",
    "score_rf =  modelFit_rf.score(X_test, y_test)\n",
    "classReport_rf = classification_report(y_test, predictions_rf)\n",
    "r2_rf = r2_score(y_test, predictions_rf)\n",
    "accuracy_rf = accuracy_score(y_test, predictions_rf)\n",
    "\n",
    "print \"Confusion Matrix:\\n\",confMatrix_rf \n",
    "print \"Classification Report:\\n\",classReport_rf\n",
    "print \"r2_score: \", r2_rf\n",
    "print \"accuracy score: \", accuracy_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treating information leakage issue\n",
    "Previously, I evaluated the model with random set of samples. Due to this it is possible the model is trained on future flight information and evalauted on past flights. This may thus leak the information about future. In reality, since the model is intended to be implemented to predict about future flights, it is therefore important to train model on past samples and test on the future samples. \n",
    "\n",
    "To understand the performance of same model with a different strategy of train and test split, I train the model on samples from 2015 and evaluated on samples corresponding to 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainSet = final_feature_test.loc[final_feature_test[\"YEAR\"]==2015] \n",
    "testSet = final_feature_test.loc[final_feature_test[\"YEAR\"]==2016]\n",
    "trainSet_features = trainSet.ix[:,0:18]\n",
    "trainSet_labels = trainSet.ix[:,18:19]\n",
    "testSet_features = testSet.ix[:,0:18]\n",
    "testSet_labels = testSet.ix[:,18:19]\n",
    "\n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(trainSet_features)\n",
    "y_train = np.array(trainSet_labels)\n",
    "X_test = np.array(testSet_features)\n",
    "y_test = np.array(testSet_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelFit_rf = RandomForestClassifier(n_estimators=50,max_features='auto',max_depth=None,min_samples_leaf=1).fit(X_train,y_train)\n",
    "predictions_rf = modelFit_rf.predict(X_test)\n",
    "confMatrix_rf = confusion_matrix(y_test, predictions_rf)\n",
    "score_rf =  modelFit_rf.score(X_test, y_test)\n",
    "classReport_rf = classification_report(y_test, predictions_rf)\n",
    "r2_rf = r2_score(y_test, predictions_rf)\n",
    "accuracy_rf = accuracy_score(y_test, predictions_rf)\n",
    "\n",
    "print \"Confusion Matrix:\\n\",confMatrix_rf \n",
    "print \"Classification Report:\\n\",classReport_rf\n",
    "print \"r2_score: \", r2_rf\n",
    "print \"accuracy score: \", accuracy_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown from the above figures, the model performance deteriorated significantly. Now model is predicting wildly almost 75% of the cancelled flights as non-cancelled flights. This damages Recall and R2 scores significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, a very interesting dataset about US domestic flights was provided. I studied the dataset and formulated the problem of flight cancellation predictions. \n",
    "\n",
    "Due to the time limitations (7 partial days) and other several competing deadlines, I had to limit this work significantly. However, there are several items that can be addressed such as:\n",
    "\n",
    "- Model development and evaluation should be more methodical and systematic. There are several machine learning algorithms, which were not tested. The algorithms that were used should also be tested with several different parameters. The evaluation should also be based on several runs of train and test splits if random sampling is used.\n",
    "- In feature extraction part, weather information of destination airports should be included.\n",
    "- Tail number of flights can be used to understand dimensions of flights. \n",
    "- The features related to text from news and twitter should be analysed using the flight dates with origin, destination and carrier tags. \n",
    "- Time series of events occurring before the flight dates can be analysed to predict the future. \n",
    "- The time window of prediction can be adjusted to understand its impact on the accuracy of prediction.\n",
    "- The impact of delay-related features on cancellation predictions should be studied.\n",
    "- The importance of features should be studied in detail.\n",
    "- Finally, this predictions can be used for all the airports to understand if the performance of model improves r not. Since the overall cancellation rate decreases if we consider all airports, the imbalance between classes will further increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print temp_df.shape\n",
    "print raw_data.shape\n",
    "raw_data['CRS_DEP_EPOCH__TS'] = temp_df['EPOCH_TS']/1000000000 # converting from nano-second to second\n",
    "raw_data['CRS_DEP_DATE_TIME'] = temp_df['DATE_TIME']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df = pd.DataFrame()\n",
    "temp_df = raw_data.copy()\n",
    "temp_df['YEAR_STR'] = raw_data['YEAR'].apply(str)\n",
    "temp_df['MONTH_STR'] = raw_data['MONTH'].apply(str)\n",
    "temp_df['DAY_STR'] = raw_data['DAY_OF_MONTH'].apply(str)\n",
    "temp_df['CRS_DEP_TIME_HOUR_STR'] = raw_data['CRS_DEP_TIME_HOUR'].apply(str)\n",
    "temp_df['DATE_TIME'] = temp_df[\"YEAR_STR\"] + \"/\" + temp_df[\"MONTH_STR\"] + \"/\" + temp_df[\"DAY_STR\"] + \":\"+ temp_df[\"CRS_DEP_TIME_HOUR_STR\"]\n",
    "temp_df['DATE_TIME'] =  pd.to_datetime(temp_df['DATE_TIME'], format='%Y/%m/%d:%H')\n",
    "temp_df['EPOCH_TS'] = pd.DatetimeIndex(temp_df['DATE_TIME']).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import forecastio\n",
    "from dateutil import rrule\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "i=0\n",
    "origin_ord_weather = pd.DataFrame(columns=('w_origin','w_timestamp','temperature', 'windSpeed', 'cloudCover', 'precipProbability'))\n",
    "ORIGINS = [\"ORD\"]\n",
    "start = datetime(2015,1,1)\n",
    "print \"start\"\n",
    "for origin in ORIGINS:\n",
    "    months_19 = start + relativedelta(months=+19)#timedelta(month=1)\n",
    "    count=0\n",
    "    for dt in rrule.rrule(rrule.DAILY, dtstart=start, until=months_19):\n",
    "        count=count+1\n",
    "        forecast = forecastio.load_forecast(api_key, lat, lng,dt,units=\"uk2\")\n",
    "        for data in forecast.json[\"hourly\"][\"data\"]:\n",
    "            origin_ord_weather.loc[i]= [origin,data.get(\"time\"),data.get(\"temperature\"),data.get(\"windSpeed\"),data.get(\"cloudCover\"),data.get(\"precipProbability\")]\n",
    "            i=i+1    #    print data.get(\"visibility\")\n",
    "            #print forecast.json[\"hourly\"]\n",
    "print \"end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print origin_ord_weather.describe()\n",
    "origin_ord_weather.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin_ord_weather[\"windSpeed\"].fillna(method='pad')\n",
    "print origin_ord_weather.dtypes\n",
    "print origin_ord_weather[\"windSpeed\"].count()\n",
    "print origin_ord_weather[\"windSpeed\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight_features_1_indexed_ord_2_pd[\"EPOCH_TS\"] = temp_df_2['EPOCH_TS']\n",
    "flight_features_1_indexed_ord_2_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_features_1_indexed_ord_2_pd[\"EPOCH_TS\"]=flight_features_1_indexed_ord_2_pd[\"EPOCH_TS\"]/1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flight_features_1_indexed_ord_2_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flight_features_1_indexed_ord_2_pd[\"EPOCH_TS_PRED\"] = flight_features_1_indexed_ord_2_pd[\"EPOCH_TS\"] - 10800 # prediction timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merge_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_feature_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_feature_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_feature_test['cloudCover'] = final_feature_test['cloudCover'].fillna(np.mean(final_feature_test['cloudCover']))\n",
    "final_feature_test['precipProbability'] = final_feature_test['precipProbability'].fillna(np.mean(final_feature_test['precipProbability']))\n",
    "final_feature_test['windSpeed'] = final_feature_test['windSpeed'].fillna(np.mean(final_feature_test['windSpeed']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features = final_feature_test.ix[:,0:18]\n",
    "labels = final_feature_test.ix[:,18:19]\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainSet = final_feature_test.loc[(final_feature_test[\"YEAR\"]==2015) | ((final_feature_test[\"YEAR\"]==2016) & (final_feature_test[\"MONTH\"]<4) ) ]\n",
    "testSet = final_feature_test.loc[(final_feature_test[\"YEAR\"]==2016) & (final_feature_test[\"MONTH\"]>4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSet_features = trainSet.ix[:,0:18]\n",
    "trainSet_labels = trainSet.ix[:,18:19]\n",
    "testSet_features = testSet.ix[:,0:18]\n",
    "testSet_labels = testSet.ix[:,18:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "temp_df_2 = pd.DataFrame()\n",
    "temp_df_2 = flight_features_1_indexed_ord_2_pd.copy()\n",
    "temp_df_2['YEAR_STR'] = temp_df_2['YEAR'].apply(str)\n",
    "temp_df_2['MONTH_STR'] = temp_df_2['MONTH'].apply(str)\n",
    "temp_df_2['DAY_STR'] = temp_df_2['DAY_OF_MONTH'].apply(str)\n",
    "temp_df_2['CRS_DEP_TIME_HOUR_STR'] = temp_df_2['CRS_DEP_TIME_HOUR'].apply(str)\n",
    "temp_df_2['DATE_TIME'] = temp_df_2[\"YEAR_STR\"] + \"/\" + temp_df_2[\"MONTH_STR\"] + \"/\" + temp_df_2[\"DAY_STR\"] + \":\"+ temp_df_2[\"CRS_DEP_TIME_HOUR_STR\"]\n",
    "temp_df_2['DATE_TIME'] =  pd.to_datetime(temp_df_2['DATE_TIME'], format='%Y/%m/%d:%H')\n",
    "temp_df_2['EPOCH_TS'] = pd.DatetimeIndex(temp_df_2['DATE_TIME'],tz='US/Central').astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "origin_ord_weather.head(100)\n",
    "print lat, lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "api_key = \"0341d3b89ededa99333e99c274163489\"\n",
    "lat = 41.9\n",
    "lng = -87.9\n",
    "\n",
    "years = [2015]\n",
    "months = [1:2]\n",
    "days = [1:]\n",
    "for year in 2015,2016\n",
    "#lat  = 52.370235\n",
    "#lng  = 4.903549\n",
    "x=2015\n",
    "y=1\n",
    "z=1\n",
    "date = datetime.datetime(x,y,z)\n",
    "\n",
    "forecast = forecastio.load_forecast(api_key, lat, lng,date,units=\"uk2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "byHour = forecast.hourly()\n",
    "#print byHour.summary\n",
    "#print byHour.icon\n",
    "\n",
    "#with open('/Users/eshjuna/Desktop/Misc/career/assignment_s/ORD_2015_weather', 'w') as outfile:\n",
    "#    json.dump(forecast.json[\"hourly\"][\"data\"], outfile)\n",
    "forecast.json[\"hourly\"]\n",
    "#for data in forecast.json[\"hourly\"][\"data\"]:\n",
    "#    print data.get(\"visibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print byHour.data\n",
    "'''\n",
    "for hourlyData in byHour.data:\n",
    "        try:\n",
    "            print hourlyData.time, hourlyData.temperature,hourlyData.precipProbability,hourlyData.visibility,hourlyData.windSpeed\n",
    "            #break\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "'''\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import forecastio\n",
    "import getpass\n",
    "\n",
    "api_key = getpass.getpass()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ORD Chicago location\n",
    "lat = 30.25\n",
    "lng = -97.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = datetime.datetime(2015,1,1)\n",
    "forecast = forecastio.load_forecast(api_key, lat, lng, time=date, units=\"us\")\n",
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df_2 =  pd.DataFrame()\n",
    "temp_df_2 = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df_2.reset_index()\n",
    "temp_df_2.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp_df_2[\"CRS_DEP_EPOCH__TS\"][:1000].apply(myTry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def myTry(m):\n",
    "    return temp_df_2[(temp_df_2[\"CRS_DEP_EPOCH__TS\"]>=m-3600) & (temp_df_2[\"CRS_DEP_EPOCH__TS\"]<m)].groupby(\"ORIGIN\")[\"CANCELLED\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    ser = temp_df_2['DIVERTED'][(temp_df_2[\"CRS_DEP_DATE_TIME\"] < x) & (temp_df_2[\"CRS_DEP_DATE_TIME\"] >= x+pd.Timedelta(hours=-2))]\n",
    "    return ser.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    ser = first_100['DIVERTED'][(first_100[\"CRS_DEP_DATE_TIME\"] < x) & (first_100[\"CRS_DEP_DATE_TIME\"] >= x+pd.Timedelta(hours=-2))]\n",
    "    return ser.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grouped = temp_df_2.groupby(\"UNIQUE_CARRIER\")\n",
    "#grouped.head(10)\n",
    "first_100 = temp_df_2.iloc[:10000]\n",
    "#first_100\n",
    "windowed_grouped = first_100[\"CRS_DEP_DATE_TIME\"].groupby(first_100[\"UNIQUE_CARRIER\"]).apply(g) #.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "windowed_grouped.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windows = temp_df_2['CANCELLED'][(temp_df_2[\"CRS_DEP_DATE_TIME\"] >= temp_df_2['CRS_DEP_DATE_TIME'] + pd.Timedelta(hours=-2)) & (temp_df_2[\"CRS_DEP_DATE_TIME\"] < temp_df_2[\"CRS_DEP_DATE_TIME\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "windows.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandasql as pdsql\n",
    "pysql = lambda q: pdsql.sqldf(q, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str1 = \"select * from temp_df_2\"\n",
    "temp_df_3 = pdsql.sqldf(str1, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df_3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_df.assign(rn=temp_df.sort_values(['DATE_TIME'], ascending=TRUE)\n",
    "                                      .groupby(['ORIGIN'])\n",
    "                .query('rn<'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tips.assign(rn=tips.sort_values(['total_bill'], ascending=False)\n",
    "   ....:                     .groupby(['day'])\n",
    "   ....:                     .cumcount() + 1)\n",
    "   ....:      .query('rn < 3')\n",
    "   ....:      .sort_values(['day','rn'])\n",
    "   ....: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = temp_df['DATE_TIME'] + pd.Timedelta(hours=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print temp_df['DATE_TIME'][i],u[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_rough = pd.DataFrame()\n",
    "#features[\"UNIQUE_CARRIER\"] = raw_data[\"UNIQUE_CARRIER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_rough = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_rough['MONTH_STR'] = features_rough['MONTH'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_rough['YEAR_STR'] = features_rough['YEAR'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_rough['DAY_STR'] = features_rough['DAY_OF_MONTH'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_rough['CRS_DEP_HOUR_STR'] = features_rough['CRS_DEP_TIME_HOUR'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del t\n",
    "t = features_rough[\"YEAR_STR\"] + \"/\" + features_rough[\"MONTH_STR\"] + \"/\" + features_rough[\"DAY_STR\"] + \":\"+ features_rough[\"CRS_DEP_HOUR_STR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#del u\n",
    "u =  pd.to_datetime(t, format='%Y/%m/%d:%H')\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = pd.DatetimeIndex(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index2 = index.astype(np.int64)\n",
    "for i in range(10):\n",
    "    print index[i],index2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('/Users/eshjuna/Desktop/Misc/career/assignment_s/flight_features_1_indexed_file.json', 'rb') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the trailing \"\\n\" from each line\n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "\n",
    "# each element of 'data' is an individual JSON object.\n",
    "# i want to convert it into an *array* of JSON objects\n",
    "# which, in and of itself, is one large JSON object\n",
    "# basically... add square brackets to the beginning\n",
    "# and end, and have all the individual business JSON objects\n",
    "# separated by a comma\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "features_1 = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example script that scrapes data from the IEM ASOS download service\n",
    "\"\"\"\n",
    "import json\n",
    "import datetime\n",
    "import urllib2\n",
    "\n",
    "# timestamps in UTC to request data for\n",
    "startts = datetime.datetime(2015, 1, 1)\n",
    "endts = datetime.datetime(2012, 1, 2)\n",
    "\n",
    "SERVICE = \"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
    "SERVICE += \"data=all&tz=Etc/UTC&format=comma&latlon=yes&\"\n",
    "\n",
    "SERVICE += startts.strftime('year1=%Y&month1=%m&day1=%d&')\n",
    "SERVICE += endts.strftime('year2=%Y&month2=%m&day2=%d&')\n",
    "\n",
    "states = \"\"\"AK AL AR AZ CA CO CT DE FL GA HI IA ID IL IN KS KY LA MA MD ME\n",
    " MI MN MO MS MT NC ND NE NH NJ NM NV NY OH OK OR PA RI SC SD TN TX UT VA VT\n",
    " WA WI WV WY\"\"\"\n",
    "# IEM quirk to have Iowa AWOS sites in its own labeled network\n",
    "networks = ['AWOS']\n",
    "for state in states.split():\n",
    "    networks.append(\"%s_ASOS\" % (state,))\n",
    "\n",
    "for network in networks:\n",
    "    # Get metadata\n",
    "    uri = \"http://mesonet.agron.iastate.edu/geojson/network.php?network=%s\" % (\n",
    "                                                                    network,)\n",
    "    data = urllib2.urlopen(uri)\n",
    "    jdict = json.load(data)\n",
    "    for site in jdict['features']:\n",
    "        faaid = site['properties']['sid']\n",
    "        sitename = site['properties']['sname']\n",
    "        uri = '%s&station=%s' % (SERVICE, faaid)\n",
    "        print 'Network: %s Downloading: %s [%s]' % (network, sitename, faaid)\n",
    "        data = urllib2.urlopen(uri)\n",
    "        outfn = '%s_%s_%s.txt' % (faaid, startts.strftime(\"%Y%m%d%H%M\"),\n",
    "                                  endts.strftime(\"%Y%m%d%H%M\"))\n",
    "        out = open(outfn, 'w')\n",
    "        out.write(data.read())\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import openweather\n",
    "from datetime import datetime\n",
    "\n",
    "# create client\n",
    "ow = openweather.OpenWeather()\n",
    "\n",
    "# find weather stations near me\n",
    "stations = ow.find_stations_near(\n",
    "    41.9,  # longitude\n",
    "    -87.9, # latitude\n",
    "    100   # kilometer radius\n",
    ")\n",
    "\n",
    "# iterate results\n",
    "for station in stations:\n",
    "    print station\n",
    "\n",
    "# get current weather at Cologne/Bonn airport\n",
    "# (station id = 4885)\n",
    "print ow.get_weather(4885)\n",
    "\n",
    "# historic weather\n",
    "start_date = datetime(2015, 01, 01)\n",
    "end_date = datetime(2015, 01, 31)\n",
    "\n",
    "# default: hourly interval\n",
    "print ow.get_historic_weather(4885, start_date, end_date)\n",
    "\n",
    "# daily aggregates\n",
    "print ow.get_historic_weather(4885, start_date, end_date, \"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "importances = modelFit_rf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in modelFit_rf.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), indices)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
